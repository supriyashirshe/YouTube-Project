{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da29b59f",
   "metadata": {},
   "source": [
    "# You Tube Data Fetch Using API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f59453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np              \n",
    "import pandas as pd        \n",
    "import IPython.display             #for displaying objects in different formats\n",
    "import googleapiclient.discovery   #allows interaction with Google APIs\n",
    "\n",
    "\n",
    "from dateutil import parser        #helps in parsing dates from strings in various formats and converting them into datetime objects in Python\n",
    "import isodate\n",
    "\n",
    "# Data visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\", color_codes=True)\n",
    "\n",
    "from googleapiclient.discovery import build        #connection to a particular Google API service\n",
    "from textblob import TextBlob            # It provides a simple API for common natural language processing (NLP) tasks such as Sentiment analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0246a878",
   "metadata": {},
   "outputs": [],
   "source": [
    "Api_key = 'API ID'     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2130fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ids = ['Video Id' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7a2098",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "    \n",
    "    # Get credentials and create an API client\n",
    "youtube = googleapiclient.discovery.build(\n",
    "        api_service_name, api_version, developerKey=Api_key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc6512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the channel stats using the channel id \n",
    "def get_channel_stats(youtube,channel_ids):\n",
    "    \n",
    "    \"\"\"\n",
    "    Get channel stats\n",
    "    \n",
    "    Params:\n",
    "    ------\n",
    "    youtube: build object of Youtube API\n",
    "    channel_ids: list of channel IDs\n",
    "    \n",
    "    Returns:\n",
    "    ------\n",
    "    dataframe with all channel stats for each channel ID\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    request = youtube.channels().list(\n",
    "        part=\"snippet,contentDetails,statistics\",\n",
    "        id=','.join(channel_ids)\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    # loop through items\n",
    "    for item in response['items']:\n",
    "        data = {'channelName': item['snippet']['title'],\n",
    "                'subscribers': item['statistics']['subscriberCount'],\n",
    "                'views': item['statistics']['viewCount'],\n",
    "                'totalVideos': item['statistics']['videoCount'],\n",
    "                'playlistId': item['contentDetails']['relatedPlaylists']['uploads']\n",
    "        }\n",
    "        \n",
    "        all_data.append(data)\n",
    "        \n",
    "    return pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4697daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_stats = get_channel_stats(youtube,channel_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3739e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292c8362",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the videos_id by playlist ID\n",
    "playlist_id = channel_stats[\"playlistId\"][0]\n",
    "def get_video_ids(youtube, playlist_id):\n",
    "    \n",
    "    video_ids = []\n",
    "    \n",
    "    request = youtube.playlistItems().list(\n",
    "        part=\"snippet,contentDetails\",\n",
    "        playlistId=playlist_id,\n",
    "        maxResults = 50\n",
    "    )\n",
    "    response = request.execute()\n",
    "    \n",
    "    for item in response['items']:\n",
    "        video_ids.append(item['contentDetails']['videoId'])\n",
    "        \n",
    "    next_page_token = response.get('nextPageToken')\n",
    "    while next_page_token is not None:\n",
    "        request = youtube.playlistItems().list(\n",
    "                    part='contentDetails',\n",
    "                    playlistId = playlist_id,\n",
    "                    maxResults = 50,\n",
    "                    pageToken = next_page_token)\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response['items']:\n",
    "            video_ids.append(item['contentDetails']['videoId'])\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        \n",
    "    return video_ids\n",
    "        \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0672368",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_ids = get_video_ids(youtube,playlist_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a248c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(video_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96911a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the video detail by video_id\n",
    "def get_video_details(youtube, video_ids):\n",
    "\n",
    "    all_video_info = []\n",
    "    \n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        request = youtube.videos().list(\n",
    "            part=\"snippet,contentDetails,statistics\",\n",
    "            id=','.join(video_ids[i:i+50])\n",
    "        )\n",
    "        response = request.execute() \n",
    "\n",
    "        for video in response['items']:\n",
    "            stats_to_keep = {'snippet': ['channelTitle', 'title', 'description', 'tags', 'publishedAt'],\n",
    "                             'statistics': ['viewCount', 'likeCount', 'favouriteCount', 'commentCount'],\n",
    "                             'contentDetails': ['duration', 'definition', 'caption'],\n",
    "                            }\n",
    "            video_info = {}\n",
    "            video_info['video_id'] = video['id']\n",
    "\n",
    "            for k in stats_to_keep.keys():\n",
    "                for v in stats_to_keep[k]:\n",
    "                    try:\n",
    "                        video_info[v] = video[k][v]\n",
    "                    except:\n",
    "                        video_info[v] = None\n",
    "\n",
    "            all_video_info.append(video_info)\n",
    "    \n",
    "    return pd.DataFrame(all_video_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4325278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get video details\n",
    "video_df = get_video_details(youtube, video_ids)\n",
    "video_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5d91a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the data in csv format\n",
    "video_df.to_csv('Video_Details(Vlad and Niki).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c346b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the comments\n",
    "def get_comments_in_videos(youtube, video_ids):\n",
    "    \"\"\"\n",
    "    Get top level comments as text from all videos with given IDs (only the first 10 comments due to quote limit of Youtube API)\n",
    "    Params:\n",
    "    \n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    video_ids: list of video IDs\n",
    "    \n",
    "    Returns:\n",
    "    Dataframe with video IDs and associated top level comment in text.\n",
    "    \n",
    "    \"\"\"\n",
    "    all_comments = []\n",
    "    \n",
    "    for video_id in video_ids:\n",
    "        try:   \n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet,replies\",\n",
    "                videoId=video_id\n",
    "            )\n",
    "            response = request.execute()\n",
    "        \n",
    "            comments_in_video = [comment['snippet']['topLevelComment']['snippet']['textOriginal'] for comment in response['items'][0:10]]\n",
    "            comments_in_video_info = {'video_id': video_id, 'comments': comments_in_video}\n",
    "\n",
    "            all_comments.append(comments_in_video_info)\n",
    "            \n",
    "        except: \n",
    "            # When error occurs - most likely because comments are disabled on a video\n",
    "            print('Could not get comments for video ' + video_id)\n",
    "        \n",
    "    return pd.DataFrame(all_comments)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33394eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = get_comments_in_videos(youtube, video_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cba84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(comment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e2bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Comment\n",
    "#Extract the data in csv format\n",
    "comment_df.to_csv('comment(MrBeast).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e525fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8a04fce",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd2ab9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc73178",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Video_Details(PewDiePie).csv',lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c98ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"favouriteCount\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4026e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c261f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description'].fillna(\"No description\",inplace=True)\n",
    "df['tags'].fillna(\"No tags\",inplace = True)\n",
    "\n",
    "df['commentCount'].fillna(0,inplace= True)\n",
    "df['likeCount'].fillna(df['likeCount'].median(),inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810fd3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4e4d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['viewCount', 'likeCount', 'commentCount']\n",
    "df[cols] = df[cols].apply(pd.to_numeric, errors='coerce', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357f9613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create publish day (in the week) column\n",
    "df['publishedAt'] =  df['publishedAt'].apply(lambda x: parser.parse(x)) \n",
    "df['pushblishDayName'] = df['publishedAt'].apply(lambda x: x.strftime(\"%A\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba8a8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert duration to seconds\n",
    "df['durationSecs'] = df['duration'].apply(lambda x: isodate.parse_duration(x))\n",
    "df['durationSecs'] = df['durationSecs'].astype('timedelta64[s]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46352b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add number of tags\n",
    "df['tagsCount'] = df['tags'].apply(lambda x: 0 if x is None else len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6bb2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comments and likes per 1000 view ratio\n",
    "df['likeRatio'] = df['likeCount']/ df['viewCount'] * 1000\n",
    "df['commentRatio'] = df['commentCount']/ df['viewCount'] * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89d78b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title character length\n",
    "df['titleLength'] = df['title'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84386046",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Video_Details_clean_file(PewDiePie).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b11200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9a7155c",
   "metadata": {},
   "source": [
    "# Data Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7288e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of file paths for the CSV files\n",
    "file_paths = [\n",
    "  \"E:\\\\Great learning All Data Analytise Material\\\\final live project\\\\Comedy_youtube_data.csv\",\n",
    "  \"E:\\\\Great learning All Data Analytise Material\\\\final live project\\\\Films_youtube_data.csv\",\n",
    "  \"E:\\\\Great learning All Data Analytise Material\\\\final live project\\\\Finance_youtube_data.csv\",\n",
    "  \"E:\\\\Great learning All Data Analytise Material\\\\final live project\\\\Shopping_youtube_data.csv\",\n",
    "  \"E:\\\\Great learning All Data Analytise Material\\\\final live project\\\\Gaming_youtube_data.csv\"\n",
    "]\n",
    "\n",
    "# Read each CSV file and store them in a list of dataframes\n",
    "dfs = []\n",
    "for file_path in file_paths:\n",
    "    df = pd.read_csv(file_path)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "concatenated_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Write the concatenated dataframe to a new CSV file\n",
    "output_file_path = 'merged_file2.csv'\n",
    "concatenated_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(\"Merged CSV file has been created successfully at:\", output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1126a48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9577e362",
   "metadata": {},
   "source": [
    "\n",
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6042b5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set up YouTube API key and build service\n",
    "API_KEY = 'AIzaSyAYFsIiNVuar2ruuDER36UUzzYpzoh18cA'\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "def get_video_comments(video_id):\n",
    "    comments = []\n",
    "    request = youtube.commentThreads().list(\n",
    "        part=\"snippet\",\n",
    "        videoId=video_id,\n",
    "        textFormat=\"plainText\",\n",
    "        maxResults=100\n",
    "    )\n",
    "    while request:\n",
    "        response = request.execute()\n",
    "        for item in response[\"items\"]:\n",
    "            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "            comments.append(comment)\n",
    "        request = youtube.commentThreads().list_next(request, response)\n",
    "    return comments\n",
    "\n",
    "def analyze_sentiment(comments):\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    neutral = 0\n",
    "    total_comments = len(comments)\n",
    "    \n",
    "    for comment in comments:\n",
    "        analysis = TextBlob(comment)\n",
    "        if analysis.sentiment.polarity > 0:\n",
    "            positive += 1\n",
    "        elif analysis.sentiment.polarity < 0:\n",
    "            negative += 1\n",
    "        else:\n",
    "            neutral += 1\n",
    "    \n",
    "    # Calculate percentages\n",
    "    positive_percent = (positive / total_comments) * 100\n",
    "    negative_percent = (negative / total_comments) * 100\n",
    "    neutral_percent = (neutral / total_comments) * 100\n",
    "    \n",
    "    sentiment_percentages = {\n",
    "        'positive': positive_percent,\n",
    "        'negative': negative_percent,\n",
    "        'neutral': neutral_percent\n",
    "    }\n",
    "    return sentiment_percentages\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_id = \"YlvcFJOE-OE\"\n",
    "    comments = get_video_comments(video_id)\n",
    "    sentiment_percentages = analyze_sentiment(comments)\n",
    "    print(\"Sentiment Analysis Results:\")\n",
    "    print(\"Positive Comments Percentage:\", sentiment_percentages['positive'], \"%\")\n",
    "    print(\"Negative Comments Percentage:\", sentiment_percentages['negative'], \"%\")\n",
    "    print(\"Neutral Comments Percentage:\", sentiment_percentages['neutral'], \"%\")\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
